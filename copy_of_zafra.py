# -*- coding: utf-8 -*-
"""Copy of Zafra.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QucFMGVktQPrKXRB5ccY4-luHZtaaOrO
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt



import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

train=pd.read_csv('/content/drive/MyDrive/Dataset/DataStorm/train_kaggle.csv')
train

test=pd.read_csv('/content/drive/MyDrive/Dataset/DataStorm/test_kaggle.csv')
test.head()



train.info()

test.info()



"""**Comverting to numeric**"""

luxury_sales_non_numeric_rows = train[pd.to_numeric(train['luxury_sales'], errors='coerce').isna()]
fresh_sales_non_numeric_rows = train[pd.to_numeric(train['fresh_sales'], errors='coerce').isna()]
dry_sales_non_numeric_rows = train[pd.to_numeric(train['dry_sales'], errors='coerce').isna()]

print("Rows where 'luxury_sales' contains non-numeric values:")
print(luxury_sales_non_numeric_rows)

print("\nRows where 'fresh_sales' contains non-numeric values:")
print(fresh_sales_non_numeric_rows)

print("\nRows where 'dry_sales' contains non-numeric values:")
print(dry_sales_non_numeric_rows)

luxury_sales_non_numeric_rows = test[pd.to_numeric(test['luxury_sales'], errors='coerce').isna()]
fresh_sales_non_numeric_rows = test[pd.to_numeric(test['fresh_sales'], errors='coerce').isna()]
dry_sales_non_numeric_rows = test[pd.to_numeric(test['dry_sales'], errors='coerce').isna()]

print("Rows where 'luxury_sales' contains non-numeric values:")
print(luxury_sales_non_numeric_rows)

print("\nRows where 'fresh_sales' contains non-numeric values:")
print(fresh_sales_non_numeric_rows)

print("\nRows where 'dry_sales' contains non-numeric values:")
print(dry_sales_non_numeric_rows)

# Mapping dictionary for specific textual representations to numbers
text_to_number_mapping = {
    'one thousand four hundread ruppes': 1400,
    'eight hundread ruppess': 800,
    'six hundread and hirty ': 630,
    'thousand tow hundread ': 1200,
    'seven hundread and nine ruppees': 709,
    'three thousand two hundread ruppess': 3200,
    'six hundread and thirty': 630,
    'six hundread and five ruppes': 605,
    'three thousana and five hundread': 3500,
    'thirteen thousand ruppes': 13000,
    'five thousand ruppes': 5000,
    'two thousand seven hundread ruppess': 2700,
    'four thousand one hundread ruppess': 4100,
    'four thousand and two hundread ruppes': 4200,
    'seven hundread and sixty ruppees' : 760,
    'eight hundread and fifteen ruppes' : 815,
    'nine thousand ruppess' : 9000,
    'three thousand seven hundread ruppees' : 3700
}

# Function to convert specific textual representations to numbers
def text_to_number(text):
    if pd.isna(text):
        return text
    if isinstance(text, str):
        text = text.lower()
        if text == 'nul':
            return float('nan')  # Convert 'nul' to NaN
        return text_to_number_mapping.get(text, text)
    else:
        return text

train['luxury_sales'] = train['luxury_sales'].apply(text_to_number)
train['fresh_sales'] = train['fresh_sales'].apply(text_to_number)
train['dry_sales'] = train['dry_sales'].apply(text_to_number)

test['luxury_sales'] = test['luxury_sales'].apply(text_to_number)
test['fresh_sales'] = test['fresh_sales'].apply(text_to_number)
test['dry_sales'] = test['dry_sales'].apply(text_to_number)

"""**Missing values**"""

def get_numerical_summary(df):
    total = train.shape[0]
    missing_columns = [col for col in train.columns if train[col].isnull().sum() > 0]
    missing_percent = {}
    for col in missing_columns:
        null_count = train[col].isnull().sum()
        per = (null_count/total) * 100
        missing_percent[col] = per
        print("{} : {} ({}%)".format(col, null_count, round(per, 3)))
    return missing_percent

get_numerical_summary(train)

train = train.dropna()
test = test.dropna()

train.isnull().sum()

test.isnull().sum()



"""**Coverting to lower case**"""

train['outlet_city'].value_counts()

test['outlet_city'].value_counts()

train['outlet_city'] = train['outlet_city'].str.lower()

test['outlet_city'] = test['outlet_city'].str.lower()

test['outlet_city'].value_counts()

test['outlet_city'] = test['outlet_city'].replace('trincomale', 'trincomalee')

train['cluster_catgeory'].value_counts()

train['cluster_catgeory'] = train['cluster_catgeory'].astype(str)
train['cluster_catgeory'] = train['cluster_catgeory'].str.strip().str.replace('\\', '')
train['cluster_catgeory'] = train['cluster_catgeory'].replace(['100.0', '98', '95', '89.0', '99'], np.nan)
train['cluster_catgeory'] = pd.to_numeric(train['cluster_catgeory'], errors='coerce')
cluster_category_counts = train['cluster_catgeory'].value_counts()
train = train.dropna()

print(cluster_category_counts)

"""**Changing data types**"""

train['Customer_ID'] = train['Customer_ID'].astype(int)
train['luxury_sales'] = train['luxury_sales'].astype(float)
train['fresh_sales'] = train['fresh_sales'].astype(float)
train['dry_sales'] = train['dry_sales'].astype(float)
train['outlet_city'] = train['outlet_city'].astype('category')
train['cluster_catgeory'] = train['cluster_catgeory'].astype('int')

test['Customer_ID'] = test['Customer_ID'].astype(int)
test['luxury_sales'] = test['luxury_sales'].astype(float)
test['fresh_sales'] = test['fresh_sales'].astype(float)
test['dry_sales'] = test['dry_sales'].astype(float)
test['outlet_city'] = test['outlet_city'].astype('category')



train.info()

test.info()

test_seen = test[~test['outlet_city'].isin(['anuradhapura', 'madavachchiya'])]
test_unseen = test[test['outlet_city'].isin(['anuradhapura', 'madavachchiya'])]

test_unseen.info()

"""**Duplicates**"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

y = train['cluster_catgeory']
le = LabelEncoder()
y_encoded = le.fit_transform(y)

train = train.drop(['Customer_ID', 'cluster_catgeory','outlet_city'], axis=1)
test_seen1 = test_seen.drop(['Customer_ID','outlet_city'], axis=1)
test_unseen1 = test_unseen.drop(['Customer_ID','outlet_city'], axis=1)

test_seen

from sklearn.preprocessing import OneHotEncoder,LabelEncoder, RobustScaler

duplicates_train=train[train.duplicated()].shape[0]
duplicates_test=test[test.duplicated()].shape[0]

print(duplicates_train)
print(duplicates_test)

X_train, X_test, y_train, y_test = train_test_split(train, y_encoded, test_size=0.2, random_state=42)

X_train

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

xgb_model = XGBClassifier()

# Fit the pipeline on the training data
xgb_model.fit(X_train, y_train)

# Predict the target values for the test data
y_pred = xgb_model.predict(X_test)
y_pred_tra = xgb_model.predict(X_train)

# Calculate accuracy score
accuracy_tra = accuracy_score(y_train, y_pred_tra)
accuracy_te = accuracy_score(y_test, y_pred)

print("Accuracy Score train:", accuracy_tra)
print("Accuracy Score test:", accuracy_te)

#0.9997739164136684

test_unseen
results_df.drop_duplicates(inplace=True)
results_df

y_pred = xgb_model.predict(test_seen1)
y_pred_original = le.inverse_transform(y_pred)

y_pred1 = xgb_model.predict(test_unseen1)
y_pred_original1 = le.inverse_transform(y_pred1)

results_df = pd.DataFrame({'Customer_ID': test_seen['Customer_ID'], 'cluster category': y_pred_original})
results_df['cluster category'].value_counts()

results_df1 = pd.DataFrame({'Customer_ID': test_unseen['Customer_ID'], 'cluster category': y_pred_original1})
results_df1['cluster category'].value_counts()

results_df=pd.concat([results_df,results_df1])
results_df

from google.colab import files

files.download('sub7.csv')

aa = pd.read_csv('/content/predicted_clusters_test.csv')
aa

merged_df = pd.merge(results_df, aa, on='Customer_ID', how='outer')

merged_df

merged_df.to_csv('sub9.csv', index=False)

from google.colab import files

files.download('sub9.csv')



merged_df['is_same'] = merged_df['cluster category'] == merged_df['Predicted_Cluster_Category']
different_values = merged_df[~merged_df['is_same']]
print(different_values)

test[test['Customer_ID']==38489]

test[test['Customer_ID']==31118]



test[test['Customer_ID']==19675]



test_scaled

exclude_values = ['x0_anuradhapura', 'x0_madawachiya	']

# Filter the DataFrame
filtered_test = test_scaled[['x0_anuradhapura', 'x0_madawachiya']]
filtered_test

y_pred = xgb_model.predict(filtered_test)
y_pred_original = le.inverse_transform(y_pred)


results_dffiltered = pd.DataFrame({'Customer ID': test['Customer_ID'], 'cluster category': y_pred_original})
results_df['fil']=results_dffiltered['cluster category']
results_df